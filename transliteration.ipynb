{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bab0c68-fd02-402b-b514-2b3ad83d1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models,preprocessing\n",
    "from tensorflow.keras.utils import plot_model,to_categorical\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,Embedding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57842bf0-afc1-4842-b922-ea7fde58679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13578101-86dc-48f8-80ce-4e36beee3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transliteration_dataset.txt','r',encoding='utf-8') as f:\n",
    "    rows=f.read().split('\\n')\n",
    "    for row in rows[0:30823]:        \n",
    "        input_text,target_text = row.split('\\t')   \n",
    "        target_text='\\t' + target_text + '\\n'\n",
    "        input_texts.append(input_text.lower())\n",
    "        target_texts.append(target_text.lower())    \n",
    "        input_characters.update(list(input_text.lower()))\n",
    "        target_characters.update(list(target_text.lower()))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61fbc47a-c076-4808-ae0d-c1a17ef2fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbea0479-45a6-4db6-a796-38f250d21797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_en_chars = len(input_characters)\n",
    "num_dec_chars = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0458c5e2-0ff6-442a-a2a7-41420c0c1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_input_length = max([len(i) for i in input_texts])\n",
    "max_target_length = max([len(i) for i in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d484266e-3744-40ac-9c54-c82a1dd76a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacters(input_texts,target_texts):\n",
    "  \n",
    "  en_in_data=[] ; dec_in_data=[] ; dec_tr_data=[]\n",
    "  \n",
    "  pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "  pad_dec=[0]*(len(target_characters)) ; pad_dec[2]=1\n",
    "  \n",
    "  cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char')\n",
    "  for i,(input_t,target_t) in enumerate(zip(input_texts,target_texts)):\n",
    "    \n",
    "    cv_inp= cv.fit(input_characters)\n",
    "    \n",
    "   \n",
    "    en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "    cv_tar= cv.fit(target_characters)\t\t\n",
    "    dec_in_data.append(cv_tar.transform(list(target_t)).toarray().tolist())\n",
    "    \n",
    "    dec_tr_data.append(cv_tar.transform(list(target_t)[1:]).toarray().tolist())\n",
    "    \n",
    "    \n",
    "    if len(input_t) < max_input_length:\n",
    "      for _ in range(max_input_length-len(input_t)):\n",
    "        en_in_data[i].append(pad_en)\n",
    "    if len(target_t) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)):\n",
    "        dec_in_data[i].append(pad_dec)\n",
    "    if (len(target_t)-1) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)+1):\n",
    "        dec_tr_data[i].append(pad_dec)\n",
    "  \n",
    " \n",
    "  en_in_data=np.array(en_in_data,dtype=\"float32\")\n",
    "  dec_in_data=np.array(dec_in_data,dtype=\"float32\")\n",
    "  dec_tr_data=np.array(dec_tr_data,dtype=\"float32\")\n",
    "  en_in_data\n",
    "\n",
    "  return en_in_data,dec_in_data,dec_tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240e9819-5fe8-40ab-8d3c-9dff64e06933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_inputs = Input(shape=(None, num_en_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b39eff-1472-48f7-882e-923153373b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = LSTM(256, return_state=True,return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fca9ed-f896-45e7-ad35-1c37dcb9a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a65d9f29-301b-41e6-a8c2-88b519055264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287fe3d6-b6eb-48a5-bc6e-66c04ffeecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc93951-b09b-4b51-85b4-1bcd7c5a65a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dec_inputs = Input(shape=(None, num_dec_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d2e5f3e-1c87-4317-8305-8f4df0ad4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding( num_dec_chars, 256 , mask_zero=True) (dec_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1d8412-cb99-461a-8a17-b2c6d9b7a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dec_lstm = LSTM(256, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76276b65-4283-48b6-bada-eb59debeed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ba55c5-5384-4e01-8c88-e1fe93a1e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dec_dense = Dense(num_dec_chars, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8cc5fac-629a-4312-ac0b-d8fed8093937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model([en_inputs, dec_inputs], dec_outputs)\n",
    "pickle.dump({'input_characters':input_characters,'target_characters':target_characters,\n",
    "             'max_input_length':max_input_length,'max_target_length':max_target_length,\n",
    "             'num_en_chars':num_en_chars,'num_dec_chars':num_dec_chars},open(\"training_data_transliteration.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a337cf-591e-498c-8d49-5e9a8616d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:551: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_in_data,dec_in_data,dec_tr_data = bagofcharacters(input_texts,target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c4cc99-f2f0-4d63-8512-fc559f2c6a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49316/49316 [==============================] - 5942s 120ms/step - loss: 0.1306 - accuracy: 0.9593 - val_loss: 0.1130 - val_accuracy: 0.9639\n",
      "Epoch 2/5\n",
      "49316/49316 [==============================] - 5894s 120ms/step - loss: 0.0806 - accuracy: 0.9740 - val_loss: 0.0848 - val_accuracy: 0.9728\n",
      "Epoch 3/5\n",
      "49316/49316 [==============================] - 35727s 724ms/step - loss: 0.0571 - accuracy: 0.9803 - val_loss: 0.0633 - val_accuracy: 0.9783\n",
      "Epoch 5/5\n",
      "49316/49316 [==============================] - 20762s 421ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.0596 - val_accuracy: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 27)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 68)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 256),  290816      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  332800      input_2[0][0]                    \n",
      "                                                                 lstm[2][1]                       \n",
      "                                                                 lstm[2][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 68)     17476       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 641,092\n",
      "Trainable params: 641,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history=model.fit(\n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data,\n",
    "    batch_size=1,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "model.save(\"s2s_transliteration\")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a92ba-4cf2-4302-b130-5b00c727a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def decode_sequence(input_seq):\n",
    "        datafile = pickle.load(open(\"training_data_transliteration.pkl\",\"rb\"))       \n",
    "        target_characters = datafile['target_characters']        \n",
    "        max_target_length = datafile['max_target_length']           \n",
    "        model = models.load_model(\"s2s_transliteration\")    \n",
    "        enc_outputs, state_h_enc, state_c_enc = model.layers[2].output         \n",
    "        en_model = Model(model.input[0], [state_h_enc, state_c_enc])\n",
    "        dec_state_input_h = Input(shape=(256,), name=\"input_6\")\n",
    "        dec_state_input_c = Input(shape=(256,), name=\"input_5\")\n",
    "        dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
    "        dec_lstm = model.layers[3]\n",
    "        dec_outputs, state_h_dec, state_c_dec = dec_lstm(model.input[1], initial_state=dec_states_inputs )       \n",
    "        dec_states = [state_h_dec, state_c_dec]\n",
    "        dec_dense = model.layers[4]\n",
    "        dec_outputs = dec_dense(dec_outputs)\n",
    "        dec_model = Model([model.input[1]] + dec_states_inputs, [dec_outputs] + dec_states   )\n",
    "        reverse_target_char_index = dict(enumerate(target_characters))        \n",
    "        states_value = en_model.predict(input_seq)       \n",
    "        cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char')        \n",
    "        co=cv.fit(target_characters) \n",
    "        target_seq=np.array([co.transform(list(\"\\t\")).toarray().tolist()],dtype=\"float32\")        \n",
    "        stop_condition = False        \n",
    "        decoded_sentence = \"\"\n",
    "        while not stop_condition:           \n",
    "            output_chars, h, c = dec_model.predict([target_seq] + states_value)            \n",
    "            char_index = np.argmax(output_chars[0, -1, :])\n",
    "            text_char = reverse_target_char_index[char_index]\n",
    "            decoded_sentence += text_char            \n",
    "            if text_char == \"\\n\" or len(decoded_sentence) > max_target_length:\n",
    "                stop_condition = True           \n",
    "            target_seq = np.zeros((1, 1, num_dec_chars))\n",
    "            target_seq[0, 0, char_index] = 1.0\n",
    "            states_value = [h, c]        \n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa19bb-4e02-4eaa-bad8-654e7fa8cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacter(input_t):\n",
    "        datafile = pickle.load(open(\"training_data_transliteration.pkl\",\"rb\"))\n",
    "        input_characters = datafile['input_characters']        \n",
    "        max_input_length = datafile['max_input_length']       \n",
    "        cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char') \n",
    "        en_in_data=[] ; pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "        cv_inp= cv.fit(input_characters)\n",
    "        en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "        if len(input_t)< max_input_length:\n",
    "            for _ in range(max_input_length-len(input_t)):\n",
    "                en_in_data[0].append(pad_en)    \n",
    "        return np.array(en_in_data,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fdcd4-8beb-4804-945a-b27b9e0695ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):   \n",
    "    str1 = \"\"   \n",
    "    for ele in s:\n",
    "        str1 += ele    \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c17f9-a0e6-41d1-9f03-3e7f019b80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry():\n",
    "    x=  input( 'Enter eng sentence : ' ) \n",
    "    input_text = x.split(' ') \n",
    "    count=0\n",
    "    output_texts=\"\"\n",
    "    \n",
    "    for x in input_text:\n",
    "        en_in_data = bagofcharacter(x.lower()+\".\")    \n",
    "        x=decode_sequence(en_in_data)\n",
    "        output_texts+=\" \"+ x\n",
    "        print(output_texts)\n",
    "           \n",
    "          \n",
    "    \n",
    "       \n",
    "    print(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75aa5b2-f6d1-4694-9e96-515aa86fe378",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_texts=\"\"\n",
    "\n",
    "entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544010a-66a0-4f3e-9206-103eea7ace37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72846a82-6ff2-4d8b-a646-eed5011630f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f1782-406e-455d-8231-649b837a4e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
